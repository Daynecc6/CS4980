{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Total Cost: 1378.63\n",
      "Average Training Time: 37.29s\n",
      "Average Solve Time: 0.00s\n",
      "Average Total Penalty: 2978.61\n",
      "Average Early Violations: 0.80\n",
      "Average Late Violations: 8.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from data_generation import generate_data\n",
    "from visualizations import plot_tour_graph\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim, hidden_dim=128):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.softmax(self.network(x), dim=-1)\n",
    "\n",
    "class ConstrainedRLAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=0.001, gamma=0.99, beta=0.1, alpha=0.01):\n",
    "        self.policy_net = PolicyNetwork(state_dim, action_dim)\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.beta = beta\n",
    "        self.alpha = alpha  # Learning rate for Lagrange multiplier\n",
    "        self.lambda_penalty = 0.0  # Initialize Lagrange multiplier\n",
    "\n",
    "    def select_action(self, state, mask, epsilon):\n",
    "        state_t = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        raw_probs = self.policy_net(state_t).squeeze(0)\n",
    "        mask_t = torch.tensor(mask, dtype=torch.float32)\n",
    "        masked_probs = raw_probs * mask_t\n",
    "\n",
    "        if masked_probs.sum().item() <= 1e-12:  # Handle zero-probability case\n",
    "            available_actions = torch.nonzero(mask_t, as_tuple=False).flatten()\n",
    "            uniform_probs = torch.zeros_like(mask_t)\n",
    "            uniform_probs[available_actions] = 1.0 / len(available_actions)\n",
    "            masked_probs = uniform_probs\n",
    "        else:\n",
    "            masked_probs /= masked_probs.sum()\n",
    "\n",
    "        if np.random.rand() < epsilon:\n",
    "            available_actions = np.nonzero(mask)[0]\n",
    "            action = np.random.choice(available_actions)\n",
    "            log_prob = torch.log(masked_probs[action] + 1e-8)\n",
    "        else:\n",
    "            m = Categorical(masked_probs)\n",
    "            action = m.sample()\n",
    "            log_prob = m.log_prob(action)\n",
    "\n",
    "        return action.item(), log_prob\n",
    "\n",
    "    def compute_loss(self, rewards, log_probs, constraints):\n",
    "        discounted_rewards = []\n",
    "        cumulative = 0.0\n",
    "        for r in reversed(rewards):\n",
    "            cumulative = r + self.gamma * cumulative\n",
    "            discounted_rewards.insert(0, cumulative)\n",
    "        discounted_rewards = torch.tensor(discounted_rewards, dtype=torch.float32)\n",
    "\n",
    "        log_probs = torch.stack(log_probs)\n",
    "        constraints = torch.stack(constraints)\n",
    "\n",
    "        policy_loss = -(log_probs * discounted_rewards).mean()\n",
    "        constraint_loss = constraints.mean()\n",
    "        loss = policy_loss + self.lambda_penalty * constraint_loss\n",
    "        return loss, constraint_loss.item()\n",
    "\n",
    "    def update_policy(self, rewards, log_probs, constraints, actual_violation):\n",
    "        loss, constraint_loss = self.compute_loss(rewards, log_probs, constraints)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Update Lagrange multiplier dynamically\n",
    "        self.lambda_penalty = max(0.0, self.lambda_penalty + self.alpha * (actual_violation))\n",
    "\n",
    "def environment_step(state, action, travel_time, time_windows, visited, penalty_scale=2.0):\n",
    "    current_city = state['current_city']\n",
    "    current_time = state['current_time']\n",
    "\n",
    "    travel_cost = travel_time[current_city, action]\n",
    "    arrival_time = current_time + travel_cost\n",
    "    earliest, latest = time_windows[action]\n",
    "\n",
    "    violation = 0\n",
    "    early_violation = 0\n",
    "    late_violation = 0\n",
    "\n",
    "    if arrival_time < earliest:\n",
    "        violation = penalty_scale * (earliest - arrival_time)\n",
    "        early_violation = 1\n",
    "        arrival_time = earliest\n",
    "    elif arrival_time > latest:\n",
    "        violation = penalty_scale * (arrival_time - latest)\n",
    "        late_violation = 1\n",
    "\n",
    "    next_state = {\n",
    "        'current_city': action,\n",
    "        'current_time': arrival_time\n",
    "    }\n",
    "    visited[action] = True\n",
    "    done = all(visited)\n",
    "\n",
    "    reward = -(travel_cost + violation)\n",
    "    return next_state, reward, violation, early_violation, late_violation, done\n",
    "\n",
    "def compute_route_cost(route, travel_time, time_windows):\n",
    "    current_time = 0\n",
    "    total_cost = 0\n",
    "    for i in range(len(route) - 1):\n",
    "        current_city = route[i]\n",
    "        next_city = route[i + 1]\n",
    "        t_time = travel_time[current_city, next_city]\n",
    "        current_time += t_time\n",
    "\n",
    "        earliest, latest = time_windows[next_city]\n",
    "        penalty = 0\n",
    "        if current_time < earliest:\n",
    "            penalty = earliest - current_time\n",
    "            current_time = earliest\n",
    "        elif current_time > latest:\n",
    "            penalty = current_time - latest\n",
    "\n",
    "        total_cost += t_time + penalty\n",
    "    return total_cost\n",
    "\n",
    "def main():\n",
    "    num_cases = 5\n",
    "    num_cities = 10\n",
    "\n",
    "    total_costs = []\n",
    "    total_penalties = []\n",
    "    total_early_violations = []\n",
    "    total_late_violations = []\n",
    "    solve_times = []\n",
    "    training_times = []  # New list to track training times\n",
    "\n",
    "    for test_id in range(num_cases):\n",
    "        coords, time_windows, travel_time = generate_data(num_cities, seed=test_id)\n",
    "\n",
    "        state_dim = 2  # current_city, current_time\n",
    "        action_dim = num_cities\n",
    "\n",
    "        agent = ConstrainedRLAgent(state_dim, action_dim)\n",
    "        best_route = None\n",
    "        best_total_reward = float('-inf')\n",
    "\n",
    "        # Track training time\n",
    "        training_start_time = time.time()\n",
    "        for episode in range(5000):\n",
    "            visited = [False] * num_cities\n",
    "            visited[0] = True\n",
    "            state = {'current_city': 0, 'current_time': 0}\n",
    "            route = [0]\n",
    "            rewards, log_probs, violations = [], [], []\n",
    "            early_violations, late_violations = 0, 0\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                mask = [0 if v else 1 for v in visited]\n",
    "                s = np.array([state['current_city'], state['current_time']], dtype=np.float32)\n",
    "\n",
    "                epsilon = max(0.1, 1.0 - episode / 5000)\n",
    "                action, log_prob = agent.select_action(s, mask, epsilon)\n",
    "\n",
    "                next_state, reward, penalty, early, late, done = environment_step(\n",
    "                    state, action, travel_time, time_windows, visited\n",
    "                )\n",
    "\n",
    "                rewards.append(reward)\n",
    "                log_probs.append(log_prob)\n",
    "                violations.append(torch.tensor(penalty, dtype=torch.float32))\n",
    "                early_violations += early\n",
    "                late_violations += late\n",
    "\n",
    "                route.append(action)\n",
    "                state = next_state\n",
    "\n",
    "            actual_violation = sum(v.item() for v in violations)  # Total violations\n",
    "            agent.update_policy(rewards, log_probs, violations, actual_violation)\n",
    "            total_reward = sum(rewards)\n",
    "\n",
    "            if total_reward > best_total_reward:\n",
    "                best_route = route\n",
    "                best_total_reward = total_reward\n",
    "\n",
    "        training_time = time.time() - training_start_time  # End training time\n",
    "        training_times.append(training_time)\n",
    "\n",
    "        # Track solve time\n",
    "        solve_start_time = time.time()\n",
    "        if best_route[-1] != 0:\n",
    "            best_route.append(0)\n",
    "\n",
    "        total_cost = compute_route_cost(best_route, travel_time, time_windows)\n",
    "        solve_time = time.time() - solve_start_time  # End solve time\n",
    "\n",
    "        total_penalty = sum(v.item() for v in violations)\n",
    "\n",
    "        total_costs.append(total_cost)\n",
    "        total_penalties.append(total_penalty)\n",
    "        total_early_violations.append(early_violations)\n",
    "        total_late_violations.append(late_violations)\n",
    "        solve_times.append(solve_time)\n",
    "\n",
    "    print(f\"Average Total Cost: {np.mean(total_costs):.2f}\")\n",
    "    print(f\"Average Training Time: {np.mean(training_times):.2f}s\")  # Output training time\n",
    "    print(f\"Average Solve Time: {np.mean(solve_times):.2f}s\")\n",
    "    print(f\"Average Total Penalty: {np.mean(total_penalties):.2f}\")\n",
    "    print(f\"Average Early Violations: {np.mean(total_early_violations):.2f}\")\n",
    "    print(f\"Average Late Violations: {np.mean(total_late_violations):.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
